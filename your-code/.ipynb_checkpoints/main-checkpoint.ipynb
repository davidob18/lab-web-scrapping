{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy\n",
    "\n",
    "# from pprint import pprint\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "response = requests.get(url)\n",
    "sopa_git = BeautifulSoup(response.content)\n",
    "bloques = sopa_git.select('article.Box-row.d-flex')\n",
    "usuarios = [bloque.select('div.col-md-6:first-of-type')[0] for bloque in bloques ]\n",
    "nombres = [usuario.select('h1 a')[0].text.strip(' \\n') for usuario in usuarios]\n",
    "usuarios_nick = [usuario.select('p a')[0].text.strip(' \\n') for usuario in usuarios] # se truena debido a que ahi tablas donde no tiene nick name de usuario \n",
    "respuesta = [f'{user} ({name})' if user != name else user for name, user in zip(nombres, usuarios_nick)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ajcvickers (Arthur Vickers)',\n",
       " 'sorrycc (chencheng (云谦))',\n",
       " 'sobolevn (Nikita Sobolev)',\n",
       " 'monkeyWie (Li Wei)',\n",
       " 'Mause (Elliana May)',\n",
       " 'basnijholt (Bas Nijholt)',\n",
       " 'emilk (Emil Ernerfeldt)',\n",
       " 'pcuenca (Pedro Cuenca)',\n",
       " 'stephencelis (Stephen Celis)',\n",
       " 'fffonion (Wangchong Zhou)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "respuesta[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hpcaitech/ColossalAI',\n",
       " 'vinta/awesome-python',\n",
       " 'PaddlePaddle/PaddleDetection',\n",
       " 'bregman-arie/devops-exercises',\n",
       " '521xueweihan/HelloGitHub',\n",
       " 'pre-commit/pre-commit-hooks',\n",
       " 'public-apis/public-apis',\n",
       " 'microsoft/recommenders',\n",
       " 'd8ahazard/sd_dreambooth_extension',\n",
       " 'open-mmlab/mmdeploy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "response_2 = requests.get(url2)\n",
    "sopa_git_2 = BeautifulSoup(response_2.content)\n",
    "bloques_2 = sopa_git_2.select('article.Box-row')\n",
    "repo_usuario = [repo.select('h1 a')[0].text.strip('\\n ').replace(\"\\n\\n    \", \"\").replace(\" \", \"\") for repo in bloques_2] \n",
    "repo_usuario[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://avatars.githubusercontent.com/u/31818963?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/5706969?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/23111350?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/72588413?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/72907851?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/652070?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/33116358?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/4560482?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/596651?s=40&v=4',\n",
       " 'https://avatars.githubusercontent.com/u/178641?s=40&v=4']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "response_3 = requests.get(url3)\n",
    "sopa_git_3 = BeautifulSoup(response_3.content)\n",
    "bloques_3 = sopa_git_2.select('a img')\n",
    "repo_link = [imag['src'] for imag in bloques_3 if bool(imag['src'])]\n",
    "repo_link[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "links=[]\n",
    "response_4 = requests.get(url4)\n",
    "sopa_git_4 = BeautifulSoup(response_4.content)\n",
    "bloques_4= sopa_git_4.select('li a')\n",
    "\n",
    "for link in bloques_4:\n",
    "    links.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Snakes',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All titles in the format selected compressed into a zip archive.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url5 ='http://uscode.house.gov/download/download.shtml' \n",
    "response_4 = requests.get(url5)\n",
    "sopa_git_4 = BeautifulSoup(response_4.content, 'html5lib') #sopa = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "bloques_5= sopa_git_4.select('div.uscitem') #('div.usctitle')\n",
    "lista_title = []\n",
    "for info in bloques_5:\n",
    "    try:\n",
    "        lista_title.append(info.select('div.usctitle , div.usctitlechanged')[0].text.strip('\\n ').replace(\" ٭\", \"\"))\n",
    "    except:\n",
    "        x=0\n",
    "lista_title.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 1 - General Provisions',\n",
       " 'Title 2 - The Congress',\n",
       " 'Title 3 - The President',\n",
       " 'Title 4 - Flag and Seal, Seat of Government, and the States',\n",
       " 'Title 5 - Government Organization and Employees',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 7 - Agriculture',\n",
       " 'Title 8 - Aliens and Nationality',\n",
       " 'Title 9 - Arbitration',\n",
       " 'Title 10 - Armed Forces',\n",
       " 'Title 11 - Bankruptcy',\n",
       " 'Title 12 - Banks and Banking',\n",
       " 'Title 13 - Census',\n",
       " 'Title 14 - Coast Guard',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 17 - Copyrights',\n",
       " 'Title 18 - Crimes and Criminal Procedure',\n",
       " 'Title 19 - Customs Duties',\n",
       " 'Title 20 - Education',\n",
       " 'Title 21 - Food and Drugs',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 23 - Highways',\n",
       " 'Title 24 - Hospitals and Asylums',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 26 - Internal Revenue Code',\n",
       " 'Title 27 - Intoxicating Liquors',\n",
       " 'Title 28 - Judiciary and Judicial Procedure',\n",
       " 'Title 29 - Labor',\n",
       " 'Title 30 - Mineral Lands and Mining',\n",
       " 'Title 31 - Money and Finance',\n",
       " 'Title 32 - National Guard',\n",
       " 'Title 33 - Navigation and Navigable Waters',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " 'Title 35 - Patents',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations',\n",
       " 'Title 37 - Pay and Allowances of the Uniformed Services',\n",
       " \"Title 38 - Veterans' Benefits\",\n",
       " 'Title 39 - Postal Service',\n",
       " 'Title 40 - Public Buildings, Property, and Works',\n",
       " 'Title 41 - Public Contracts',\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 43 - Public Lands',\n",
       " 'Title 44 - Public Printing and Documents',\n",
       " 'Title 45 - Railroads',\n",
       " 'Title 46 - Shipping',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 48 - Territories and Insular Possessions',\n",
       " 'Title 49 - Transportation',\n",
       " 'Title 50 - War and National Defense',\n",
       " 'Title 51 - National and Commercial Space Programs',\n",
       " 'Title 52 - Voting and Elections',\n",
       " 'Title 53 [Reserved]',\n",
       " 'Title 54 - National Park Service and Related Programs']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'RUJA IGNATOVA',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "url6 ='https://www.fbi.gov/wanted/topten' \n",
    "response_6 = requests.get(url6)\n",
    "sopa_git_6 = BeautifulSoup(response_6.content, 'html5lib') #sopa = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "bloques_6= sopa_git_6.select('li.portal-type-person.castle-grid-block-item') #('div.usctitle')\n",
    "\n",
    "nombres_bus = [nombre.text.strip('\\n ') for nombre in bloques_6]\n",
    "nombres_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LUGAR</th>\n",
       "      <th>latituf</th>\n",
       "      <th>longitud</th>\n",
       "      <th>dia y tiempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "      <td>39.55</td>\n",
       "      <td>122.97</td>\n",
       "      <td>2022-11-1401:31:16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUNDA STRAIT, INDONESIA</td>\n",
       "      <td>6.53</td>\n",
       "      <td>105.92</td>\n",
       "      <td>2022-11-1401:13:16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HALMAHERA, INDONESIA</td>\n",
       "      <td>2.27</td>\n",
       "      <td>129.40</td>\n",
       "      <td>2022-11-1401:06:05.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PERU-ECUADOR BORDER REGION</td>\n",
       "      <td>4.86</td>\n",
       "      <td>80.76</td>\n",
       "      <td>2022-11-1400:54:02.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUERTO RICO</td>\n",
       "      <td>18.00</td>\n",
       "      <td>66.90</td>\n",
       "      <td>2022-11-1400:34:49.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>23.15</td>\n",
       "      <td>67.15</td>\n",
       "      <td>2022-11-1400:25:28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OFFSHORE COQUIMBO, CHILE</td>\n",
       "      <td>32.15</td>\n",
       "      <td>71.78</td>\n",
       "      <td>2022-11-1400:13:27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "      <td>43.98</td>\n",
       "      <td>13.34</td>\n",
       "      <td>2022-11-1323:44:42.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BANDA SEA</td>\n",
       "      <td>6.21</td>\n",
       "      <td>130.65</td>\n",
       "      <td>2022-11-1323:35:20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>19.43</td>\n",
       "      <td>155.26</td>\n",
       "      <td>2022-11-1323:25:07.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>19.18</td>\n",
       "      <td>155.42</td>\n",
       "      <td>2022-11-1323:13:47.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "      <td>12.44</td>\n",
       "      <td>87.78</td>\n",
       "      <td>2022-11-1323:04:22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PHILIPPINE ISLANDS REGION</td>\n",
       "      <td>12.89</td>\n",
       "      <td>126.04</td>\n",
       "      <td>2022-11-1322:54:20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OFFSHORE VALPARAISO, CHILE</td>\n",
       "      <td>32.17</td>\n",
       "      <td>71.77</td>\n",
       "      <td>2022-11-1322:28:35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PAKISTAN</td>\n",
       "      <td>31.65</td>\n",
       "      <td>73.02</td>\n",
       "      <td>2022-11-1322:12:17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OFFSHORE COQUIMBO, CHILE</td>\n",
       "      <td>32.15</td>\n",
       "      <td>71.88</td>\n",
       "      <td>2022-11-1322:05:41.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>35.69</td>\n",
       "      <td>120.34</td>\n",
       "      <td>2022-11-1321:49:32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>38.35</td>\n",
       "      <td>27.17</td>\n",
       "      <td>2022-11-1321:42:10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FIJI REGION</td>\n",
       "      <td>20.13</td>\n",
       "      <td>178.01</td>\n",
       "      <td>2022-11-1321:35:29.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "      <td>35.46</td>\n",
       "      <td>3.67</td>\n",
       "      <td>2022-11-1321:21:12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         LUGAR latituf longitud          dia y tiempo\n",
       "0          NORTHERN CALIFORNIA   39.55   122.97  2022-11-1401:31:16.6\n",
       "1      SUNDA STRAIT, INDONESIA    6.53   105.92  2022-11-1401:13:16.0\n",
       "2         HALMAHERA, INDONESIA    2.27   129.40  2022-11-1401:06:05.0\n",
       "3   PERU-ECUADOR BORDER REGION    4.86    80.76  2022-11-1400:54:02.0\n",
       "4                  PUERTO RICO   18.00    66.90  2022-11-1400:34:49.6\n",
       "5           ANTOFAGASTA, CHILE   23.15    67.15  2022-11-1400:25:28.0\n",
       "6     OFFSHORE COQUIMBO, CHILE   32.15    71.78  2022-11-1400:13:27.0\n",
       "7                CENTRAL ITALY   43.98    13.34  2022-11-1323:44:42.5\n",
       "8                    BANDA SEA    6.21   130.65  2022-11-1323:35:20.0\n",
       "9     ISLAND OF HAWAII, HAWAII   19.43   155.26  2022-11-1323:25:07.7\n",
       "10    ISLAND OF HAWAII, HAWAII   19.18   155.42  2022-11-1323:13:47.1\n",
       "11     NEAR COAST OF NICARAGUA   12.44    87.78  2022-11-1323:04:22.0\n",
       "12   PHILIPPINE ISLANDS REGION   12.89   126.04  2022-11-1322:54:20.0\n",
       "13  OFFSHORE VALPARAISO, CHILE   32.17    71.77  2022-11-1322:28:35.0\n",
       "14                    PAKISTAN   31.65    73.02  2022-11-1322:12:17.2\n",
       "15    OFFSHORE COQUIMBO, CHILE   32.15    71.88  2022-11-1322:05:41.2\n",
       "16          CENTRAL CALIFORNIA   35.69   120.34  2022-11-1321:49:32.6\n",
       "17              WESTERN TURKEY   38.35    27.17  2022-11-1321:42:10.2\n",
       "18                 FIJI REGION   20.13   178.01  2022-11-1321:35:29.8\n",
       "19         STRAIT OF GIBRALTAR   35.46     3.67  2022-11-1321:21:12.9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url7 ='https://www.emsc-csem.org/Earthquake/'\n",
    "response_7 = requests.get(url7)\n",
    "sopa_git_7 = BeautifulSoup(response_7.content, 'html5lib') #sopa = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "bloques_7= sopa_git_7.select('tr.ligne1') #('div.usctitle')\n",
    "\n",
    "lista_final=[]\n",
    "lista_t=[]\n",
    "for item in bloques_7:\n",
    "    lugar = item.select('td.tb_region')[0].text.replace(\"\\xa0\", \"\") \n",
    "    latitud = item.select('td.tabev1')[0].text.replace(\"\\xa0\", \"\") \n",
    "    longitud = item.select('td.tabev1')[1].text.replace(\"\\xa0\", \"\")\n",
    "    date_time = item.select('b a')[0].text.replace(\"\\xa0\", \"\") \n",
    "    lista_t.append(lugar)\n",
    "    lista_t.append(latitud)\n",
    "    lista_t.append(longitud)\n",
    "    lista_t.append(date_time)\n",
    "    lista_final.append(lista_t)\n",
    "    lista_t=[]\n",
    "\n",
    "df = pd.DataFrame(lista_final, columns=['LUGAR','latituf','longitud','dia y tiempo'])\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://mlh.io/seasons/na-2020/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>pais</th>\n",
       "      <th>dia inicio</th>\n",
       "      <th>dia termina</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HackMTY</td>\n",
       "      <td>Monterrey</td>\n",
       "      <td>MX</td>\n",
       "      <td>2019-08-24</td>\n",
       "      <td>2019-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Citizen Hacks</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>ON</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PennApps</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hackathon de Futuras Tecnologías</td>\n",
       "      <td>Torreón</td>\n",
       "      <td>MX</td>\n",
       "      <td>2019-09-07</td>\n",
       "      <td>2019-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hack the North</td>\n",
       "      <td>Waterloo</td>\n",
       "      <td>ON</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>2019-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HopHacks</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>MD</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>2019-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BigRed//Hacks</td>\n",
       "      <td>Ithaca</td>\n",
       "      <td>NY</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HackRice</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SBUHacks</td>\n",
       "      <td>Stony Brook</td>\n",
       "      <td>NY</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ShellHacks</td>\n",
       "      <td>Miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sunhacks</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>2019-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kent Hack Enough</td>\n",
       "      <td>Kent</td>\n",
       "      <td>OH</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MedHacks</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>MD</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VolHacks</td>\n",
       "      <td>Knoxville</td>\n",
       "      <td>TN</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GirlHacks</td>\n",
       "      <td>Newark</td>\n",
       "      <td>NJ</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GrizzHacks 4</td>\n",
       "      <td>Rochester</td>\n",
       "      <td>MI</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hackNY</td>\n",
       "      <td>New York City</td>\n",
       "      <td>NY</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HackUMBC</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>MD</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RamHacks</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>VA</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>2019-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HackTheU</td>\n",
       "      <td>Salt Lake City</td>\n",
       "      <td>UT</td>\n",
       "      <td>2019-10-05</td>\n",
       "      <td>2019-10-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              titulo          ciudad pais  dia inicio  \\\n",
       "0                            HackMTY       Monterrey   MX  2019-08-24   \n",
       "1                      Citizen Hacks         Toronto   ON  2019-09-06   \n",
       "2                           PennApps    Philadelphia   PA  2019-09-06   \n",
       "3   Hackathon de Futuras Tecnologías         Torreón   MX  2019-09-07   \n",
       "4                     Hack the North        Waterloo   ON  2019-09-13   \n",
       "5                           HopHacks       Baltimore   MD  2019-09-13   \n",
       "6                      BigRed//Hacks          Ithaca   NY  2019-09-20   \n",
       "7                           HackRice         Houston   TX  2019-09-20   \n",
       "8                           SBUHacks     Stony Brook   NY  2019-09-20   \n",
       "9                         ShellHacks           Miami   FL  2019-09-20   \n",
       "10                          sunhacks           Tempe   AZ  2019-09-20   \n",
       "11                  Kent Hack Enough            Kent   OH  2019-09-27   \n",
       "12                          MedHacks       Baltimore   MD  2019-09-27   \n",
       "13                          VolHacks       Knoxville   TN  2019-09-27   \n",
       "14                         GirlHacks          Newark   NJ  2019-09-28   \n",
       "15                      GrizzHacks 4       Rochester   MI  2019-09-28   \n",
       "16                            hackNY   New York City   NY  2019-09-28   \n",
       "17                          HackUMBC       Baltimore   MD  2019-09-28   \n",
       "18                          RamHacks        Richmond   VA  2019-09-28   \n",
       "19                          HackTheU  Salt Lake City   UT  2019-10-05   \n",
       "\n",
       "   dia termina  \n",
       "0   2019-08-25  \n",
       "1   2019-09-08  \n",
       "2   2019-09-08  \n",
       "3   2019-09-08  \n",
       "4   2019-09-15  \n",
       "5   2019-09-15  \n",
       "6   2019-09-22  \n",
       "7   2019-09-21  \n",
       "8   2019-09-21  \n",
       "9   2019-09-22  \n",
       "10  2019-09-22  \n",
       "11  2019-09-29  \n",
       "12  2019-09-29  \n",
       "13  2019-09-29  \n",
       "14  2019-09-29  \n",
       "15  2019-09-29  \n",
       "16  2019-09-29  \n",
       "17  2019-09-29  \n",
       "18  2019-09-29  \n",
       "19  2019-10-06  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url8 ='https://mlh.io/seasons/na-2020/events'\n",
    "response_8 = requests.get(url8)\n",
    "sopa_git_8 = BeautifulSoup(response_8.content, 'html5lib') #sopa = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "bloques_8= sopa_git_8.select('div.col-lg-3.col-md-4.col-sm-6') #('div.usctitle')\n",
    "\n",
    "lista_final=[]\n",
    "lista_t=[]\n",
    "for item in bloques_8:\n",
    "    titulo = item.select('h3.event-name')[0].text.strip('\\n ')\n",
    "    ciudad = item.select('div span:first-of-type')[0].text\n",
    "    pais = item.select('div span:nth-of-type(2)')[0].text\n",
    "    date_inicio = item.select('div meta:first-of-type')[0]['content']\n",
    "    date_termina = item.select('div meta:nth-of-type(2)')[0]['content']\n",
    "    lista_t.append(titulo)\n",
    "    lista_t.append(ciudad)\n",
    "    lista_t.append(pais)\n",
    "    lista_t.append(date_inicio)\n",
    "    lista_t.append(date_termina)\n",
    "    lista_final.append(lista_t)\n",
    "    lista_t=[]\n",
    "\n",
    "df = pd.DataFrame(lista_final, columns=['titulo','ciudad','pais','dia inicio','dia termina'])\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "def extraer_info_tw():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    import time \n",
    "    usuario = input('ingresar usuario ejemplo \"@BarackObama\":').replace(\"@\", \"\") \n",
    "    #driver = webdriver.Chrome()\n",
    "    url = f'https://twitter.com/{usuario}'\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        sopa = BeautifulSoup(driver.page_source)\n",
    "        bloques2 = sopa.select('div div a span span:first-of-type')\n",
    "        cantidad = bloques2[2].text.replace('\\xa0',' ').replace(',','.')\n",
    "        driver.quit()\n",
    "        return print(f'personas siguiendo: {cantidad}')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return print('NO encontro cuenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": shakira\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas siguiendo: 233\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": pabloescobar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO encontro cuenta\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": pabloescobar2_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas siguiendo: 0\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "def extraer_info_tw_2():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    import time \n",
    "    usuario = input('ingresar usuario ejemplo \"@BarackObama\":').replace(\"@\", \"\") \n",
    "    #driver = webdriver.Chrome()\n",
    "    url = f'https://twitter.com/{usuario}'\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        sopa = BeautifulSoup(driver.page_source)\n",
    "        bloques2 = sopa.select('div div a span span:first-of-type')\n",
    "        cantidad = bloques2[4].text.replace('\\xa0',' ').replace(',','.')\n",
    "        driver.quit()\n",
    "        return print(f'personas que la siguen: {cantidad}')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return print('NO encontro cuenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": shakira\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas que la siguen: 53.3 M\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": pabloescobar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO encontro cuenta\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": pabloescobar2_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO encontro cuenta\n"
     ]
    }
   ],
   "source": [
    "extraer_info_tw_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idioma</th>\n",
       "      <th>cantidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>6,458,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1,314,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Español</td>\n",
       "      <td>1,755,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1,798,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Français</td>\n",
       "      <td>2,400,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2,667,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1,742,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1,256,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>فارسی</td>\n",
       "      <td>866,000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Português</td>\n",
       "      <td>1,085,000+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idioma    cantidad\n",
       "0    English  6,458,000+\n",
       "1        日本語  1,314,000+\n",
       "2    Español  1,755,000+\n",
       "3    Русский  1,798,000+\n",
       "4   Français  2,400,000+\n",
       "5    Deutsch  2,667,000+\n",
       "6   Italiano  1,742,000+\n",
       "7         中文  1,256,000+\n",
       "8      فارسی    866,000+\n",
       "9  Português  1,085,000+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url11 ='https://www.wikipedia.org/'\n",
    "response_11 = requests.get(url11)\n",
    "sopa_git_11 = BeautifulSoup(response_11.content, 'html5lib') #sopa = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "bloques_11= sopa_git_11.select('div.central-featured-lang') #('div.usctitle')\n",
    "\n",
    "lista_final=[]\n",
    "lista_t=[]\n",
    "for item in bloques_11:\n",
    "    idioma = item.select('div strong')[0].text\n",
    "    cantidad = item.select('small bdi')[0].text.replace(\"\\xa0\", \",\") \n",
    "\n",
    "    lista_t.append(idioma)\n",
    "    lista_t.append(cantidad)\n",
    "\n",
    "    lista_final.append(lista_t)\n",
    "    lista_t=[]\n",
    "\n",
    "df = pd.DataFrame(lista_final, columns=['idioma','cantidad'])\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime and justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Government spending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Towns and cities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Digital service performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Government reference data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TIPOS\n",
       "0          Business and economy\n",
       "1             Crime and justice\n",
       "2                       Defence\n",
       "3                     Education\n",
       "4                   Environment\n",
       "5                    Government\n",
       "6           Government spending\n",
       "7                        Health\n",
       "8                       Mapping\n",
       "9                       Society\n",
       "10             Towns and cities\n",
       "11                    Transport\n",
       "12  Digital service performance\n",
       "13    Government reference data"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "\n",
    "#your code\n",
    "url12 ='https://www.data.gov.uk/'\n",
    "response_12 = requests.get(url12)\n",
    "sopa_git_12 = BeautifulSoup(response_12.content, 'html5lib') \n",
    "bloques_12= sopa_git_12.select('ul.govuk-list.dgu-topics__list') \n",
    "bloques_12_2 = bloques_12[0].select('li') \n",
    "\n",
    "lista_final=[]\n",
    "lista_t=[]\n",
    "for item in bloques_12_2:\n",
    "    idioma = item.select('h3')[0].text\n",
    "\n",
    "    lista_t.append(idioma)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(lista_t, columns=['TIPOS'])\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Percentageof worldpopulation(2018)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>12.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>6.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>5.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>5.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>3.5%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank          Language Percentageof worldpopulation(2018)\n",
       "0     1  Mandarin Chinese                              12.3%\n",
       "1     2           Spanish                               6.0%\n",
       "2     3           English                               5.1%\n",
       "3     3            Arabic                               5.1%\n",
       "4     5             Hindi                               3.5%"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "url13 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "response_13 = requests.get(url13)\n",
    "sopa_git_13 = BeautifulSoup(response_13.content, 'html5lib') \n",
    "info_tabla = str(sopa_git_13.select('table'))\n",
    "df = pd.read_html(info_tabla)[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "def extraer_info_tw_t():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    lista_twit=[]\n",
    "    import time \n",
    "    usuario = input('ingresar usuario ejemplo \"@BarackObama\":').replace(\"@\", \"\") \n",
    "    #driver = webdriver.Chrome()\n",
    "    url = f'https://twitter.com/{usuario}'\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        chains = ActionChains(driver)\n",
    "        for _ in range(4):\n",
    "            chains.scroll_by_amount(0, 500).perform()\n",
    "            time.sleep(1.5)\n",
    "        sopa = BeautifulSoup(driver.page_source)\n",
    "        bloques1 = sopa.select('div[data-testid=\"cellInnerDiv\"]')\n",
    "        for item in bloques1:\n",
    "            twit = item.text\n",
    "            lista_twit.append(twit)\n",
    "        driver.quit()\n",
    "        print(\"Listo\")\n",
    "        return lista_twit \n",
    "    except:\n",
    "        driver.quit()\n",
    "        return print('NO encontro cuenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ingresar usuario ejemplo \"@BarackObama\": shakira\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo\n"
     ]
    }
   ],
   "source": [
    "lista = extraer_info_tw_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tweet fijadoShakira@shakira·20 oct.#Monotonia @ozuna  YA DISPONIBLE / OUT NOW https://sml.lnk.to/Monotonia277,2\\xa0mil reproducciones0:01 / 0:338632.95818,6\\xa0mil'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
